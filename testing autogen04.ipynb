{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autogen-agentchat\n",
      "  Using cached autogen_agentchat-0.4.7-py3-none-any.whl (66 kB)\n",
      "Collecting autogen-ext[azure,openai]\n",
      "  Using cached autogen_ext-0.4.7-py3-none-any.whl (161 kB)\n",
      "Collecting autogen-core==0.4.7\n",
      "  Using cached autogen_core-0.4.7-py3-none-any.whl (81 kB)\n",
      "Requirement already satisfied: pillow>=11.0.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from autogen-core==0.4.7->autogen-agentchat) (11.1.0)\n",
      "Collecting jsonref~=1.1.0\n",
      "  Using cached jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
      "Collecting protobuf~=5.29.3\n",
      "  Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from autogen-core==0.4.7->autogen-agentchat) (2.10.6)\n",
      "Collecting opentelemetry-api>=1.27.0\n",
      "  Using cached opentelemetry_api-1.30.0-py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from autogen-core==0.4.7->autogen-agentchat) (4.12.2)\n",
      "Collecting azure-core\n",
      "  Using cached azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
      "Collecting azure-ai-inference>=1.0.0b7\n",
      "  Using cached azure_ai_inference-1.0.0b9-py3-none-any.whl (124 kB)\n",
      "Collecting azure-identity\n",
      "  Using cached azure_identity-1.20.0-py3-none-any.whl (188 kB)\n",
      "Requirement already satisfied: tiktoken>=0.8.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from autogen-ext[azure,openai]) (0.9.0)\n",
      "Collecting aiofiles\n",
      "  Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: openai>=1.52.2 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from autogen-ext[azure,openai]) (1.64.0)\n",
      "Collecting isodate>=0.6.1\n",
      "  Using cached isodate-0.7.2-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from azure-core->autogen-ext[azure,openai]) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from azure-core->autogen-ext[azure,openai]) (1.17.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from openai>=1.52.2->autogen-ext[azure,openai]) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from openai>=1.52.2->autogen-ext[azure,openai]) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from openai>=1.52.2->autogen-ext[azure,openai]) (0.8.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from openai>=1.52.2->autogen-ext[azure,openai]) (0.28.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from openai>=1.52.2->autogen-ext[azure,openai]) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from openai>=1.52.2->autogen-ext[azure,openai]) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from tiktoken>=0.8.0->autogen-ext[azure,openai]) (2024.11.6)\n",
      "Collecting cryptography>=2.5\n",
      "  Using cached cryptography-44.0.1-cp39-abi3-win_amd64.whl (3.2 MB)\n",
      "Collecting msal>=1.30.0\n",
      "  Using cached msal-1.31.1-py3-none-any.whl (113 kB)\n",
      "Collecting msal-extensions>=1.2.0\n",
      "  Using cached msal_extensions-1.2.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.52.2->autogen-ext[azure,openai]) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.52.2->autogen-ext[azure,openai]) (3.10)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from cryptography>=2.5->azure-identity->autogen-ext[azure,openai]) (1.17.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.52.2->autogen-ext[azure,openai]) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.52.2->autogen-ext[azure,openai]) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.52.2->autogen-ext[azure,openai]) (0.14.0)\n",
      "Collecting PyJWT[crypto]<3,>=1.0.0\n",
      "  Using cached PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Collecting portalocker<3,>=1.4\n",
      "  Using cached portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Collecting deprecated>=1.2.6\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Collecting importlib-metadata<=8.5.0,>=6.0\n",
      "  Using cached importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.4.7->autogen-agentchat) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.4.7->autogen-agentchat) (2.27.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core->autogen-ext[azure,openai]) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core->autogen-ext[azure,openai]) (3.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from tqdm>4->openai>=1.52.2->autogen-ext[azure,openai]) (0.4.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity->autogen-ext[azure,openai]) (2.22)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Using cached wrapt-1.17.2-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Collecting zipp>=3.20\n",
      "  Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from portalocker<3,>=1.4->msal-extensions>=1.2.0->azure-identity->autogen-ext[azure,openai]) (308)\n",
      "Installing collected packages: zipp, wrapt, PyJWT, protobuf, portalocker, jsonref, isodate, aiofiles, importlib-metadata, deprecated, cryptography, azure-core, opentelemetry-api, azure-ai-inference, msal, autogen-core, msal-extensions, autogen-ext, autogen-agentchat, azure-identity\n",
      "Successfully installed PyJWT-2.10.1 aiofiles-24.1.0 autogen-agentchat-0.4.7 autogen-core-0.4.7 autogen-ext-0.4.7 azure-ai-inference-1.0.0b9 azure-core-1.32.0 azure-identity-1.20.0 cryptography-44.0.1 deprecated-1.2.18 importlib-metadata-8.5.0 isodate-0.7.2 jsonref-1.1.0 msal-1.31.1 msal-extensions-1.2.0 opentelemetry-api-1.30.0 portalocker-2.10.1 protobuf-5.29.3 wrapt-1.17.2 zipp-3.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install -U \"autogen-agentchat\" \"autogen-ext[openai,azure]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "What is the weather in New York?\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'registry.ollama.ai/library/deepseek-r1:14b does not support tools', 'type': 'api_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 61\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m Console(agent\u001b[38;5;241m.\u001b[39mrun_stream(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the weather in New York?\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# NOTE: if running this inside a Python script you'll need to use asyncio.run(main()).\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "Cell \u001b[1;32mIn[8], line 57\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m Console(agent\u001b[38;5;241m.\u001b[39mrun_stream(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the weather in New York?\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\Pictures\\Docker\\.venv\\lib\\site-packages\\autogen_agentchat\\ui\\_console.py:117\u001b[0m, in \u001b[0;36mConsole\u001b[1;34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[0m\n\u001b[0;32m    113\u001b[0m last_processed: Optional[T] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    115\u001b[0m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[0;32m    119\u001b[0m         duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\Pictures\\Docker\\.venv\\lib\\site-packages\\autogen_agentchat\\agents\\_base_chat_agent.py:176\u001b[0m, in \u001b[0;36mBaseChatAgent.run_stream\u001b[1;34m(self, task, cancellation_token)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid message type in sequence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(msg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 176\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_messages_stream(input_messages, cancellation_token):\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, Response):\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m message\u001b[38;5;241m.\u001b[39mchat_message\n",
      "File \u001b[1;32m~\\Pictures\\Docker\\.venv\\lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:405\u001b[0m, in \u001b[0;36mAssistantAgent.on_messages_stream\u001b[1;34m(self, messages, cancellation_token)\u001b[0m\n\u001b[0;32m    402\u001b[0m model_result: CreateResult \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_client_stream:\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;66;03m# Stream the model client.\u001b[39;00m\n\u001b[1;32m--> 405\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_client\u001b[38;5;241m.\u001b[39mcreate_stream(\n\u001b[0;32m    406\u001b[0m         llm_messages, tools\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tools \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handoff_tools, cancellation_token\u001b[38;5;241m=\u001b[39mcancellation_token\n\u001b[0;32m    407\u001b[0m     ):\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunk, CreateResult):\n\u001b[0;32m    409\u001b[0m             model_result \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[1;32m~\\Pictures\\Docker\\.venv\\lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py:727\u001b[0m, in \u001b[0;36mBaseOpenAIChatCompletionClient.create_stream\u001b[1;34m(self, messages, tools, json_output, extra_create_args, cancellation_token, max_consecutive_empty_chunk_tolerance)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    726\u001b[0m     cancellation_token\u001b[38;5;241m.\u001b[39mlink_future(stream_future)\n\u001b[1;32m--> 727\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m stream_future\n\u001b[0;32m    728\u001b[0m choice: Union[ParsedChoice[Any], ParsedChoice[BaseModel], ChunkChoice] \u001b[38;5;241m=\u001b[39m cast(ChunkChoice, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    729\u001b[0m chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Pictures\\Docker\\.venv\\lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1927\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m   1885\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   1886\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m   1887\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1924\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m   1925\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[0;32m   1926\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m-> 1927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m   1928\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1929\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[0;32m   1930\u001b[0m             {\n\u001b[0;32m   1931\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m   1932\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m   1933\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m   1934\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m   1935\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m   1936\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m   1937\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m   1938\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m   1939\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m   1940\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m   1941\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m   1942\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m   1943\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m   1944\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m   1945\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m   1946\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m   1947\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m   1948\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m   1949\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m   1950\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m   1951\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m   1952\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m   1953\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m   1954\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m   1955\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m   1956\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m   1957\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m   1958\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m   1959\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m   1960\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m   1961\u001b[0m             },\n\u001b[0;32m   1962\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m   1963\u001b[0m         ),\n\u001b[0;32m   1964\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m   1965\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m   1966\u001b[0m         ),\n\u001b[0;32m   1967\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m   1968\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1969\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[0;32m   1970\u001b[0m     )\n",
      "File \u001b[1;32m~\\Pictures\\Docker\\.venv\\lib\\site-packages\\openai\\_base_client.py:1856\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1842\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1843\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1844\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1851\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1852\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m   1853\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1854\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1855\u001b[0m     )\n\u001b[1;32m-> 1856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[1;32m~\\Pictures\\Docker\\.venv\\lib\\site-packages\\openai\\_base_client.py:1550\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[0;32m   1547\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1548\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1551\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1552\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1553\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1554\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1555\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1556\u001b[0m )\n",
      "File \u001b[1;32m~\\Pictures\\Docker\\.venv\\lib\\site-packages\\openai\\_base_client.py:1651\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[1;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[0;32m   1648\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[0;32m   1650\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1651\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1653\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1654\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1655\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1660\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'registry.ollama.ai/library/deepseek-r1:14b does not support tools', 'type': 'api_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import asyncio \n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "# Define a model client. You can use other model client that implements\n",
    "# the `ChatCompletionClient` interface.\n",
    "\n",
    "config_list = [{\n",
    "    \"model\": \"deepseek-r1:14b\",\n",
    "    \"base_url\": \"http://localhost:11434\",\n",
    "    \"api_type\": \"ollama\"\n",
    "}]\n",
    "\n",
    "# Configure the assistant\n",
    "assistant_config = {\n",
    "    \"seed\": 42,\n",
    "    \"config_list\": config_list,\n",
    "    \"temperature\": 0,\n",
    "}\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"deepseek-r1:14b\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"placeholder\",\n",
    "    model_info={\n",
    "        \"vision\": False,\n",
    "        \"function_calling\": True,\n",
    "        \"json_output\": False,\n",
    "        \"family\": \"unknown\",\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "# Define a simple function tool that the agent can use.\n",
    "# For this example, we use a fake weather tool for demonstration purposes.\n",
    "async def get_weather(city: str) -> str:\n",
    "    \"\"\"Get the weather for a given city.\"\"\"\n",
    "    return f\"The weather in {city} is 73 degrees and Sunny.\"\n",
    "\n",
    "\n",
    "# Define an AssistantAgent with the model, tool, system message, and reflection enabled.\n",
    "# The system message instructs the agent via natural language.\n",
    "agent = AssistantAgent(\n",
    "    name=\"weather_agent\",\n",
    "    model_client=model_client,\n",
    "    tools=[get_weather],\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    reflect_on_tool_use=True,\n",
    "    model_client_stream=True,  # Enable streaming tokens from the model client.\n",
    ")\n",
    "\n",
    "\n",
    "# Run the agent and stream the messages to the console.\n",
    "async def main() -> None:\n",
    "    await Console(agent.run_stream(task=\"What is the weather in New York?\"))\n",
    "\n",
    "\n",
    "# NOTE: if running this inside a Python script you'll need to use asyncio.run(main()).\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish_reason='stop' content='<think>\\n\\n</think>\\n\\nThe capital of France is Paris.' usage=RequestUsage(prompt_tokens=10, completion_tokens=12) cached=False logprobs=None thought=None\n"
     ]
    }
   ],
   "source": [
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"deepseek-r1:14b\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"placeholder\",\n",
    "    model_info={\n",
    "        \"vision\": False,\n",
    "        \"function_calling\": False,\n",
    "        \"json_output\": False,\n",
    "        \"family\": \"unknown\",\n",
    "    },\n",
    ")\n",
    "\n",
    "response = await model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama version is 0.5.11\n"
     ]
    }
   ],
   "source": [
    "! ollama --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
