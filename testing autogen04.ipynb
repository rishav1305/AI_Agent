{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autogen-agentchat\n",
      "  Using cached autogen_agentchat-0.4.7-py3-none-any.whl (66 kB)\n",
      "Collecting autogen-ext[azure,openai]\n",
      "  Using cached autogen_ext-0.4.7-py3-none-any.whl (161 kB)\n",
      "Collecting autogen-core==0.4.7\n",
      "  Using cached autogen_core-0.4.7-py3-none-any.whl (81 kB)\n",
      "Requirement already satisfied: pillow>=11.0.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from autogen-core==0.4.7->autogen-agentchat) (11.1.0)\n",
      "Collecting jsonref~=1.1.0\n",
      "  Using cached jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
      "Collecting protobuf~=5.29.3\n",
      "  Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from autogen-core==0.4.7->autogen-agentchat) (2.10.6)\n",
      "Collecting opentelemetry-api>=1.27.0\n",
      "  Using cached opentelemetry_api-1.30.0-py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from autogen-core==0.4.7->autogen-agentchat) (4.12.2)\n",
      "Collecting azure-core\n",
      "  Using cached azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
      "Collecting azure-ai-inference>=1.0.0b7\n",
      "  Using cached azure_ai_inference-1.0.0b9-py3-none-any.whl (124 kB)\n",
      "Collecting azure-identity\n",
      "  Using cached azure_identity-1.20.0-py3-none-any.whl (188 kB)\n",
      "Requirement already satisfied: tiktoken>=0.8.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from autogen-ext[azure,openai]) (0.9.0)\n",
      "Collecting aiofiles\n",
      "  Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: openai>=1.52.2 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from autogen-ext[azure,openai]) (1.64.0)\n",
      "Collecting isodate>=0.6.1\n",
      "  Using cached isodate-0.7.2-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from azure-core->autogen-ext[azure,openai]) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from azure-core->autogen-ext[azure,openai]) (1.17.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from openai>=1.52.2->autogen-ext[azure,openai]) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from openai>=1.52.2->autogen-ext[azure,openai]) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from openai>=1.52.2->autogen-ext[azure,openai]) (0.8.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from openai>=1.52.2->autogen-ext[azure,openai]) (0.28.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from openai>=1.52.2->autogen-ext[azure,openai]) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from openai>=1.52.2->autogen-ext[azure,openai]) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from tiktoken>=0.8.0->autogen-ext[azure,openai]) (2024.11.6)\n",
      "Collecting cryptography>=2.5\n",
      "  Using cached cryptography-44.0.1-cp39-abi3-win_amd64.whl (3.2 MB)\n",
      "Collecting msal>=1.30.0\n",
      "  Using cached msal-1.31.1-py3-none-any.whl (113 kB)\n",
      "Collecting msal-extensions>=1.2.0\n",
      "  Using cached msal_extensions-1.2.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.52.2->autogen-ext[azure,openai]) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.52.2->autogen-ext[azure,openai]) (3.10)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from cryptography>=2.5->azure-identity->autogen-ext[azure,openai]) (1.17.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.52.2->autogen-ext[azure,openai]) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.52.2->autogen-ext[azure,openai]) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.52.2->autogen-ext[azure,openai]) (0.14.0)\n",
      "Collecting PyJWT[crypto]<3,>=1.0.0\n",
      "  Using cached PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Collecting portalocker<3,>=1.4\n",
      "  Using cached portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Collecting deprecated>=1.2.6\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Collecting importlib-metadata<=8.5.0,>=6.0\n",
      "  Using cached importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.4.7->autogen-agentchat) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.4.7->autogen-agentchat) (2.27.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core->autogen-ext[azure,openai]) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core->autogen-ext[azure,openai]) (3.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from tqdm>4->openai>=1.52.2->autogen-ext[azure,openai]) (0.4.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity->autogen-ext[azure,openai]) (2.22)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Using cached wrapt-1.17.2-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Collecting zipp>=3.20\n",
      "  Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from portalocker<3,>=1.4->msal-extensions>=1.2.0->azure-identity->autogen-ext[azure,openai]) (308)\n",
      "Installing collected packages: zipp, wrapt, PyJWT, protobuf, portalocker, jsonref, isodate, aiofiles, importlib-metadata, deprecated, cryptography, azure-core, opentelemetry-api, azure-ai-inference, msal, autogen-core, msal-extensions, autogen-ext, autogen-agentchat, azure-identity\n",
      "Successfully installed PyJWT-2.10.1 aiofiles-24.1.0 autogen-agentchat-0.4.7 autogen-core-0.4.7 autogen-ext-0.4.7 azure-ai-inference-1.0.0b9 azure-core-1.32.0 azure-identity-1.20.0 cryptography-44.0.1 deprecated-1.2.18 importlib-metadata-8.5.0 isodate-0.7.2 jsonref-1.1.0 msal-1.31.1 msal-extensions-1.2.0 opentelemetry-api-1.30.0 portalocker-2.10.1 protobuf-5.29.3 wrapt-1.17.2 zipp-3.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install -U \"autogen-agentchat\" \"autogen-ext[openai,azure]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "What is the weather in New York?\n"
     ]
    },
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_async/connection.py:101\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_async/connection.py:78\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connect(request)\n\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_async/connection.py:124\u001b[39m, in \u001b[36mAsyncHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_backend.connect_tcp(**kwargs)\n\u001b[32m    125\u001b[39m     trace.return_value = stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_backends/auto.py:31\u001b[39m, in \u001b[36mAutoBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._init_backend()\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.connect_tcp(\n\u001b[32m     32\u001b[39m     host,\n\u001b[32m     33\u001b[39m     port,\n\u001b[32m     34\u001b[39m     timeout=timeout,\n\u001b[32m     35\u001b[39m     local_address=local_address,\n\u001b[32m     36\u001b[39m     socket_options=socket_options,\n\u001b[32m     37\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_backends/anyio.py:113\u001b[39m, in \u001b[36mAnyIOBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    108\u001b[39m exc_map = {\n\u001b[32m    109\u001b[39m     \u001b[38;5;167;01mTimeoutError\u001b[39;00m: ConnectTimeout,\n\u001b[32m    110\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    111\u001b[39m     anyio.BrokenResourceError: ConnectError,\n\u001b[32m    112\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43manyio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfail_after\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/contextlib.py:155\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: All connection attempts failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1595\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1594\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1595\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.send(\n\u001b[32m   1596\u001b[39m         request,\n\u001b[32m   1597\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_stream_response_body(request=request),\n\u001b[32m   1598\u001b[39m         **kwargs,\n\u001b[32m   1599\u001b[39m     )\n\u001b[32m   1600\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py:393\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_async_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/contextlib.py:155\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: All connection attempts failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m Console(agent.run_stream(task=\u001b[33m\"\u001b[39m\u001b[33mWhat is the weather in New York?\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# NOTE: if running this inside a Python script you'll need to use asyncio.run(main()).\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m Console(agent.run_stream(task=\u001b[33m\"\u001b[39m\u001b[33mWhat is the weather in New York?\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/autogen_agentchat/ui/_console.py:117\u001b[39m, in \u001b[36mConsole\u001b[39m\u001b[34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[39m\n\u001b[32m    113\u001b[39m last_processed: Optional[T] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    115\u001b[39m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[32m    119\u001b[39m         duration = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/autogen_agentchat/agents/_base_chat_agent.py:176\u001b[39m, in \u001b[36mBaseChatAgent.run_stream\u001b[39m\u001b[34m(self, task, cancellation_token)\u001b[39m\n\u001b[32m    174\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    175\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid message type in sequence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(msg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_messages_stream(input_messages, cancellation_token):\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, Response):\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m message.chat_message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py:405\u001b[39m, in \u001b[36mAssistantAgent.on_messages_stream\u001b[39m\u001b[34m(self, messages, cancellation_token)\u001b[39m\n\u001b[32m    402\u001b[39m model_result: CreateResult | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model_client_stream:\n\u001b[32m    404\u001b[39m     \u001b[38;5;66;03m# Stream the model client.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model_client.create_stream(\n\u001b[32m    406\u001b[39m         llm_messages, tools=\u001b[38;5;28mself\u001b[39m._tools + \u001b[38;5;28mself\u001b[39m._handoff_tools, cancellation_token=cancellation_token\n\u001b[32m    407\u001b[39m     ):\n\u001b[32m    408\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunk, CreateResult):\n\u001b[32m    409\u001b[39m             model_result = chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/autogen_ext/models/openai/_openai_client.py:727\u001b[39m, in \u001b[36mBaseOpenAIChatCompletionClient.create_stream\u001b[39m\u001b[34m(self, messages, tools, json_output, extra_create_args, cancellation_token, max_consecutive_empty_chunk_tolerance)\u001b[39m\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    726\u001b[39m     cancellation_token.link_future(stream_future)\n\u001b[32m--> \u001b[39m\u001b[32m727\u001b[39m stream = \u001b[38;5;28;01mawait\u001b[39;00m stream_future\n\u001b[32m    728\u001b[39m choice: Union[ParsedChoice[Any], ParsedChoice[BaseModel], ChunkChoice] = cast(ChunkChoice, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    729\u001b[39m chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:1927\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1885\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1886\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1887\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1924\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1925\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   1926\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1927\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   1928\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1929\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   1930\u001b[39m             {\n\u001b[32m   1931\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   1932\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   1933\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   1934\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   1935\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   1936\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   1937\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   1938\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   1939\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   1940\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   1941\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   1942\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   1943\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   1944\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   1945\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   1946\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   1947\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   1948\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   1949\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   1950\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   1951\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   1952\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   1953\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   1954\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   1955\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   1956\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   1957\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   1958\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   1959\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   1960\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   1961\u001b[39m             },\n\u001b[32m   1962\u001b[39m             completion_create_params.CompletionCreateParams,\n\u001b[32m   1963\u001b[39m         ),\n\u001b[32m   1964\u001b[39m         options=make_request_options(\n\u001b[32m   1965\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   1966\u001b[39m         ),\n\u001b[32m   1967\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   1968\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1969\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   1970\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1862\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1849\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1850\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1857\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1858\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1859\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1860\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1861\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1862\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1556\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[39m\n\u001b[32m   1553\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1554\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1556\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m   1557\u001b[39m     cast_to=cast_to,\n\u001b[32m   1558\u001b[39m     options=options,\n\u001b[32m   1559\u001b[39m     stream=stream,\n\u001b[32m   1560\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1561\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1562\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1619\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1616\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered Exception\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1619\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry_request(\n\u001b[32m   1620\u001b[39m         input_options,\n\u001b[32m   1621\u001b[39m         cast_to,\n\u001b[32m   1622\u001b[39m         retries_taken=retries_taken,\n\u001b[32m   1623\u001b[39m         stream=stream,\n\u001b[32m   1624\u001b[39m         stream_cls=stream_cls,\n\u001b[32m   1625\u001b[39m         response_headers=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1626\u001b[39m     )\n\u001b[32m   1628\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1629\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1689\u001b[39m, in \u001b[36mAsyncAPIClient._retry_request\u001b[39m\u001b[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[39m\n\u001b[32m   1685\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m, options.url, timeout)\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m anyio.sleep(timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1689\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m   1690\u001b[39m     options=options,\n\u001b[32m   1691\u001b[39m     cast_to=cast_to,\n\u001b[32m   1692\u001b[39m     retries_taken=retries_taken + \u001b[32m1\u001b[39m,\n\u001b[32m   1693\u001b[39m     stream=stream,\n\u001b[32m   1694\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1695\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1619\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1616\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered Exception\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1619\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry_request(\n\u001b[32m   1620\u001b[39m         input_options,\n\u001b[32m   1621\u001b[39m         cast_to,\n\u001b[32m   1622\u001b[39m         retries_taken=retries_taken,\n\u001b[32m   1623\u001b[39m         stream=stream,\n\u001b[32m   1624\u001b[39m         stream_cls=stream_cls,\n\u001b[32m   1625\u001b[39m         response_headers=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1626\u001b[39m     )\n\u001b[32m   1628\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1629\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1689\u001b[39m, in \u001b[36mAsyncAPIClient._retry_request\u001b[39m\u001b[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[39m\n\u001b[32m   1685\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m, options.url, timeout)\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m anyio.sleep(timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1689\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m   1690\u001b[39m     options=options,\n\u001b[32m   1691\u001b[39m     cast_to=cast_to,\n\u001b[32m   1692\u001b[39m     retries_taken=retries_taken + \u001b[32m1\u001b[39m,\n\u001b[32m   1693\u001b[39m     stream=stream,\n\u001b[32m   1694\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1695\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1629\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1619\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry_request(\n\u001b[32m   1620\u001b[39m             input_options,\n\u001b[32m   1621\u001b[39m             cast_to,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1625\u001b[39m             response_headers=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1626\u001b[39m         )\n\u001b[32m   1628\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1631\u001b[39m log.debug(\n\u001b[32m   1632\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m, request.method, request.url, response.status_code, response.reason_phrase\n\u001b[32m   1633\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mAPIConnectionError\u001b[39m: Connection error."
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import asyncio \n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "# Define a model client. You can use other model client that implements\n",
    "# the `ChatCompletionClient` interface.\n",
    "\n",
    "config_list = [{\n",
    "    \"model\": \"deepseek-r1:14b\",\n",
    "    \"base_url\": \"http://localhost:11434\",\n",
    "    \"api_type\": \"ollama\"\n",
    "}]\n",
    "\n",
    "# Configure the assistant\n",
    "assistant_config = {\n",
    "    \"seed\": 42,\n",
    "    \"config_list\": config_list,\n",
    "    \"temperature\": 0,\n",
    "}\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"deepseek-r1:14b\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"placeholder\",\n",
    "    model_info={\n",
    "        \"vision\": False,\n",
    "        \"function_calling\": True,\n",
    "        \"json_output\": False,\n",
    "        \"family\": \"unknown\",\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "# Define a simple function tool that the agent can use.\n",
    "# For this example, we use a fake weather tool for demonstration purposes.\n",
    "async def get_weather(city: str) -> str:\n",
    "    \"\"\"Get the weather for a given city.\"\"\"\n",
    "    return f\"The weather in {city} is 73 degrees and Sunny.\"\n",
    "\n",
    "\n",
    "# Define an AssistantAgent with the model, tool, system message, and reflection enabled.\n",
    "# The system message instructs the agent via natural language.\n",
    "agent = AssistantAgent(\n",
    "    name=\"weather_agent\",\n",
    "    model_client=model_client,\n",
    "    tools=[get_weather],\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    reflect_on_tool_use=True,\n",
    "    model_client_stream=True,  # Enable streaming tokens from the model client.\n",
    ")\n",
    "\n",
    "\n",
    "# Run the agent and stream the messages to the console.\n",
    "async def main() -> None:\n",
    "    await Console(agent.run_stream(task=\"What is the weather in New York?\"))\n",
    "\n",
    "\n",
    "# NOTE: if running this inside a Python script you'll need to use asyncio.run(main()).\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "deepseek_model_client = OpenAIChatCompletionClient(\n",
    "    model=\"deepseek-r1:14b\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"placeholder\",\n",
    "    model_info={\n",
    "        \"vision\": False,\n",
    "        \"function_calling\": False,\n",
    "        \"json_output\": False,\n",
    "        \"family\": \"unknown\",\n",
    "    },\n",
    ")\n",
    "llama_model_client = OpenAIChatCompletionClient(\n",
    "    model=\"llama3.2\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"placeholder\",\n",
    "    model_info={\n",
    "        \"vision\": False,\n",
    "        \"function_calling\": True,\n",
    "        \"json_output\": False,\n",
    "        \"family\": \"unknown\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# response = await model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama version is 0.5.11\n"
     ]
    }
   ],
   "source": [
    "! ollama --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Travel Planning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner_agent = AssistantAgent(\n",
    "    \"planner_agent\",\n",
    "    model_client=deepseek_model_client,\n",
    "    description=\"A helpful assistant that can plan trips.\",\n",
    "    system_message=\"You are a helpful assistant that can suggest a travel plan for a user based on their request.\",\n",
    ")\n",
    "\n",
    "local_agent = AssistantAgent(\n",
    "    \"local_agent\",\n",
    "    model_client=deepseek_model_client,\n",
    "    description=\"A local assistant that can suggest local activities or places to visit.\",\n",
    "    system_message=\"You are a helpful assistant that can suggest authentic and interesting local activities or places to visit for a user and can utilize any context information provided.\",\n",
    ")\n",
    "\n",
    "language_agent = AssistantAgent(\n",
    "    \"language_agent\",\n",
    "    model_client=deepseek_model_client,\n",
    "    description=\"A helpful assistant that can provide language tips for a given destination.\",\n",
    "    system_message=\"You are a helpful assistant that can review travel plans, providing feedback on important/critical tips about how best to address language or communication challenges for the given destination. If the plan already includes language tips, you can mention that the plan is satisfactory, with rationale.\",\n",
    ")\n",
    "\n",
    "travel_summary_agent = AssistantAgent(\n",
    "    \"travel_summary_agent\",\n",
    "    model_client=deepseek_model_client,\n",
    "    description=\"A helpful assistant that can summarize the travel plan.\",\n",
    "    system_message=\"You are a helpful assistant that can take in all of the suggestions and advice from the other agents and provide a detailed final travel plan. You must ensure that the final plan is integrated and complete. YOUR FINAL RESPONSE MUST BE THE COMPLETE PLAN. When the plan is complete and all perspectives are integrated, you can respond with TERMINATE.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "Plan a 3 day trip to Nepal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message for planner_agent/23258b81-25a9-4c73-9abf-680f4f698da0\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n",
      "    resp = await self._pool.handle_async_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n",
      "    raise exc from None\n",
      "  File \"/usr/local/lib/python3.11/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n",
      "    response = await connection.handle_async_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.11/site-packages/httpcore/_async/connection.py\", line 78, in handle_async_request\n",
      "    stream = await self._connect(request)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/httpcore/_async/connection.py\", line 124, in _connect\n",
      "    stream = await self._network_backend.connect_tcp(**kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/httpcore/_backends/auto.py\", line 31, in connect_tcp\n",
      "    return await self._backend.connect_tcp(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/httpcore/_backends/anyio.py\", line 113, in connect_tcp\n",
      "    with map_exceptions(exc_map):\n",
      "  File \"/usr/local/lib/python3.11/contextlib.py\", line 155, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/usr/local/lib/python3.11/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.ConnectError: All connection attempts failed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1595, in _request\n",
      "    response = await self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/httpx/_client.py\", line 1629, in send\n",
      "    response = await self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n",
      "    response = await self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n",
      "    response = await self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n",
      "    response = await transport.handle_async_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py\", line 393, in handle_async_request\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"/usr/local/lib/python3.11/contextlib.py\", line 155, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.ConnectError: All connection attempts failed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 505, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/autogen_core/_base_agent.py\", line 113, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 48, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/autogen_core/_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/autogen_core/_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 53, in handle_request\n",
      "    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 416, in on_messages_stream\n",
      "    model_result = await self._model_client.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/autogen_ext/models/openai/_openai_client.py\", line 534, in create\n",
      "    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n",
      "                                                                     ^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1862, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1556, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1619, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1689, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1619, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1689, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1629, in _request\n",
      "    raise APIConnectionError(request=request) from err\n",
      "openai.APIConnectionError: Connection error.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='Plan a 3 day trip to Nepal.', type='TextMessage')], stop_reason=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "termination = TextMentionTermination(\"TERMINATE\")\n",
    "group_chat = RoundRobinGroupChat(\n",
    "    [planner_agent, local_agent, language_agent, travel_summary_agent], termination_condition=termination\n",
    ")\n",
    "await Console(group_chat.run_stream(task=\"Plan a 3 day trip to Nepal.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
      "Collecting feedparser~=6.0.10\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "     ---------------------------------------- 0.0/81.3 kB ? eta -:--:--\n",
      "     ------------------- ------------------ 41.0/81.3 kB 653.6 kB/s eta 0:00:01\n",
      "     ---------------------------- --------- 61.4/81.3 kB 550.5 kB/s eta 0:00:01\n",
      "     -------------------------------------- 81.3/81.3 kB 652.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests~=2.32.0 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from arxiv) (2.32.3)\n",
      "Collecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from requests~=2.32.0->arxiv) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rchatter\\pictures\\docker\\.venv\\lib\\site-packages (from requests~=2.32.0->arxiv) (2025.1.31)\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "  Running setup.py install for sgmllib3k: started\n",
      "  Running setup.py install for sgmllib3k: finished with status 'done'\n",
      "Successfully installed arxiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: sgmllib3k is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core.tools import FunctionTool\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define a simple function tool that the agent can use.\n",
    "# For this example, we use a fake weather tool for demonstration purposes.\n",
    "async def get_weather(city: str) -> str:\n",
    "    \"\"\"Get the weather for a given city.\"\"\"\n",
    "    return f\"The weather in {city} is 73 degrees and Sunny.\"\n",
    "\n",
    "\n",
    "# Define an AssistantAgent with the model, tool, system message, and reflection enabled.\n",
    "# The system message instructs the agent via natural language.\n",
    "agent = AssistantAgent(\n",
    "    name=\"weather_agent\",\n",
    "    model_client=llama_model_client,\n",
    "    tools=[get_weather],\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    reflect_on_tool_use=True,\n",
    "    model_client_stream=True,  # Enable streaming tokens from the model client.\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "What is the weather in New York?\n",
      "---------- weather_agent ----------\n",
      "[FunctionCall(id='call_shzg998c', arguments='{\"city\":\"New York\"}', name='get_weather')]\n",
      "---------- weather_agent ----------\n",
      "[FunctionExecutionResult(content='The weather in New York is 73 degrees and Sunny.', call_id='call_shzg998c', is_error=False)]\n",
      "---------- weather_agent ----------\n",
      "I'm a large language model, I don't have real-time access to the current weather conditions. My previous response was an error on my part.\n",
      "\n",
      "To get the most up-to-date and accurate weather information for New York, I recommend checking a reliable weather website or app, such as AccuWeather, Weather.com, or the National Weather Service (NWS). You can also check local news websites or stations for the latest forecast.\n",
      "\n",
      "Would you like me to help with anything else?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='What is the weather in New York?', type='TextMessage'), ToolCallRequestEvent(source='weather_agent', models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0), content=[FunctionCall(id='call_shzg998c', arguments='{\"city\":\"New York\"}', name='get_weather')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='weather_agent', models_usage=None, content=[FunctionExecutionResult(content='The weather in New York is 73 degrees and Sunny.', call_id='call_shzg998c', is_error=False)], type='ToolCallExecutionEvent'), TextMessage(source='weather_agent', models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0), content=\"I'm a large language model, I don't have real-time access to the current weather conditions. My previous response was an error on my part.\\n\\nTo get the most up-to-date and accurate weather information for New York, I recommend checking a reliable weather website or app, such as AccuWeather, Weather.com, or the National Weather Service (NWS). You can also check local news websites or stations for the latest forecast.\\n\\nWould you like me to help with anything else?\", type='TextMessage')], stop_reason=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await Console(agent.run_stream(task=\"What is the weather in New York?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Company Research Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autogen_ext'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mroot\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mAI_Agent\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_model_client\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI_Agent/common_functions.py:4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcolorama\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcolorama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Fore, Style\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mautogen_ext\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIChatCompletionClient\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Initialize colorama for cross-platform color support\u001b[39;00m\n\u001b[32m      8\u001b[39m colorama.init(autoreset=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'autogen_ext'"
     ]
    }
   ],
   "source": [
    "from root.AI_Agent.common_functions import get_model_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Downloading yfinance-0.2.54-py2.py3-none-any.whl (108 kB)\n",
      "     l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/108.7 kB ? eta -:--:--━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 102.4/108.7 kB 4.9 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 102.4/108.7 kB 4.9 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 102.4/108.7 kB 4.9 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 102.4/108.7 kB 4.9 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108.7/108.7 kB 639.8 kB/s eta 0:00:00\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "     l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/8.6 MB ? eta -:--:--━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.1/8.6 MB 4.3 MB/s eta 0:00:02━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.3/8.6 MB 3.9 MB/s eta 0:00:03━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.4/8.6 MB 3.3 MB/s eta 0:00:03━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.5/8.6 MB 3.4 MB/s eta 0:00:03━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.5/8.6 MB 3.1 MB/s eta 0:00:03━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/8.6 MB 2.8 MB/s eta 0:00:03━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/8.6 MB 2.6 MB/s eta 0:00:04━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/8.6 MB 2.6 MB/s eta 0:00:04━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/8.6 MB 2.6 MB/s eta 0:00:04━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/8.6 MB 2.6 MB/s eta 0:00:04━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/8.6 MB 2.6 MB/s eta 0:00:04━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/8.6 MB 2.6 MB/s eta 0:00:04━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/8.6 MB 2.6 MB/s eta 0:00:04━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/8.6 MB 2.6 MB/s eta 0:00:04━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/8.6 MB 2.6 MB/s eta 0:00:04━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/8.6 MB 2.6 MB/s eta 0:00:04━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/8.6 MB 1.1 MB/s eta 0:00:08━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/8.6 MB 1.1 MB/s eta 0:00:08━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/8.6 MB 1.1 MB/s eta 0:00:08━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/8.6 MB 1.1 MB/s eta 0:00:08━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/8.6 MB 1.1 MB/s eta 0:00:08━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/8.6 MB 1.1 MB/s eta 0:00:08━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/8.6 MB 1.1 MB/s eta 0:00:08━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/8.6 MB 723.7 kB/s eta 0:00:11━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/8.6 MB 735.5 kB/s eta 0:00:11━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/8.6 MB 733.0 kB/s eta 0:00:11━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/8.6 MB 731.0 kB/s eta 0:00:11━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.8/8.6 MB 773.0 kB/s eta 0:00:11━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.9/8.6 MB 805.1 kB/s eta 0:00:10━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.9/8.6 MB 843.1 kB/s eta 0:00:10━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/8.6 MB 887.7 kB/s eta 0:00:09━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/8.6 MB 936.7 kB/s eta 0:00:09━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/8.6 MB 983.2 kB/s eta 0:00:08━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/8.6 MB 1.0 MB/s eta 0:00:08━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/8.6 MB 1.0 MB/s eta 0:00:07━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/8.6 MB 1.1 MB/s eta 0:00:07━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/8.6 MB 1.1 MB/s eta 0:00:07━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/8.6 MB 1.1 MB/s eta 0:00:07━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/8.6 MB 1.2 MB/s eta 0:00:06━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/8.6 MB 1.2 MB/s eta 0:00:06━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/8.6 MB 1.3 MB/s eta 0:00:06━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/8.6 MB 1.3 MB/s eta 0:00:06━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/8.6 MB 1.3 MB/s eta 0:00:05━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/8.6 MB 1.4 MB/s eta 0:00:05━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/8.6 MB 1.5 MB/s eta 0:00:05━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/8.6 MB 1.5 MB/s eta 0:00:04━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/8.6 MB 1.6 MB/s eta 0:00:04━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/8.6 MB 1.6 MB/s eta 0:00:04━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/8.6 MB 1.6 MB/s eta 0:00:04━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/8.6 MB 1.6 MB/s eta 0:00:04━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/8.6 MB 1.6 MB/s eta 0:00:04━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/8.6 MB 1.6 MB/s eta 0:00:04━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/8.6 MB 1.7 MB/s eta 0:00:04━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 3.5/8.6 MB 1.8 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 3.8/8.6 MB 1.8 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 3.9/8.6 MB 1.9 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 4.2/8.6 MB 1.9 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 4.6/8.6 MB 2.1 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 4.8/8.6 MB 2.1 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 5.0/8.6 MB 2.2 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 5.4/8.6 MB 2.3 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 5.7/8.6 MB 2.4 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 5.9/8.6 MB 2.5 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 6.0/8.6 MB 2.5 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 6.3/8.6 MB 2.5 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 6.5/8.6 MB 2.6 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 6.8/8.6 MB 2.6 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 7.0/8.6 MB 2.7 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 7.2/8.6 MB 2.7 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 7.5/8.6 MB 2.8 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 7.6/8.6 MB 2.8 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 7.9/8.6 MB 2.9 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 8.1/8.6 MB 2.9 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 8.4/8.6 MB 2.9 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 8.6/8.6 MB 3.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 8.6/8.6 MB 3.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 8.6/8.6 MB 3.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 8.6/8.6 MB 3.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 8.6/8.6 MB 3.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 8.6/8.6 MB 3.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 8.6/8.6 MB 3.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 8.6/8.6 MB 3.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 8.6/8.6 MB 3.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 8.6/8.6 MB 3.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 8.6/8.6 MB 3.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 8.6/8.6 MB 3.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 8.6/8.6 MB 3.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 8.6/8.6 MB 3.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 8.6/8.6 MB 3.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 8.6/8.6 MB 3.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.6/8.6 MB 2.5 MB/s eta 0:00:00\n",
      "\u001b[?25hCollecting pytz\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "     l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/507.9 kB ? eta -:--:--━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 153.6/507.9 kB 4.8 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 327.7/507.9 kB 4.8 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 409.6/507.9 kB 3.6 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 501.8/507.9 kB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 501.8/507.9 kB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 501.8/507.9 kB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 501.8/507.9 kB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 501.8/507.9 kB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 507.9/507.9 kB 1.5 MB/s eta 0:00:00\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "     l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/16.4 MB ? eta -:--:--╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.3/16.4 MB 8.5 MB/s eta 0:00:02━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.5/16.4 MB 7.7 MB/s eta 0:00:03━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.8/16.4 MB 7.5 MB/s eta 0:00:03━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/16.4 MB 6.9 MB/s eta 0:00:03━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/16.4 MB 6.2 MB/s eta 0:00:03━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/16.4 MB 5.8 MB/s eta 0:00:03━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/16.4 MB 5.5 MB/s eta 0:00:03━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/16.4 MB 5.4 MB/s eta 0:00:03━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/16.4 MB 5.1 MB/s eta 0:00:03━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/16.4 MB 5.2 MB/s eta 0:00:03━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/16.4 MB 5.3 MB/s eta 0:00:03━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/16.4 MB 5.2 MB/s eta 0:00:03━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/16.4 MB 5.1 MB/s eta 0:00:03━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/16.4 MB 5.1 MB/s eta 0:00:03━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/16.4 MB 5.5 MB/s eta 0:00:03━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/16.4 MB 5.3 MB/s eta 0:00:03━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/16.4 MB 5.3 MB/s eta 0:00:03━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.7/16.4 MB 5.4 MB/s eta 0:00:03━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.0/16.4 MB 5.5 MB/s eta 0:00:03━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/16.4 MB 5.6 MB/s eta 0:00:03━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/16.4 MB 5.6 MB/s eta 0:00:03━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/16.4 MB 5.6 MB/s eta 0:00:03━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/16.4 MB 5.7 MB/s eta 0:00:03━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/16.4 MB 5.6 MB/s eta 0:00:03━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/16.4 MB 5.5 MB/s eta 0:00:03━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/16.4 MB 5.6 MB/s eta 0:00:02━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/16.4 MB 5.6 MB/s eta 0:00:02━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/16.4 MB 5.7 MB/s eta 0:00:02━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 6.4/16.4 MB 5.8 MB/s eta 0:00:02━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 6.6/16.4 MB 5.7 MB/s eta 0:00:02━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 6.7/16.4 MB 5.6 MB/s eta 0:00:02━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 6.9/16.4 MB 5.6 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 7.1/16.4 MB 5.5 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 7.2/16.4 MB 5.5 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 7.2/16.4 MB 5.4 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 7.2/16.4 MB 5.4 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 7.3/16.4 MB 5.1 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 7.4/16.4 MB 5.0 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 7.4/16.4 MB 4.9 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 7.5/16.4 MB 4.9 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 7.6/16.4 MB 4.8 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 7.7/16.4 MB 4.8 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 7.9/16.4 MB 4.7 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 8.0/16.4 MB 4.7 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 8.1/16.4 MB 4.7 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 8.2/16.4 MB 4.6 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 8.2/16.4 MB 4.5 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 8.2/16.4 MB 4.5 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 8.2/16.4 MB 4.5 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 8.3/16.4 MB 4.3 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 8.4/16.4 MB 4.3 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 8.5/16.4 MB 4.2 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 8.5/16.4 MB 4.1 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 8.6/16.4 MB 4.1 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 8.6/16.4 MB 4.0 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 8.7/16.4 MB 4.0 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 8.8/16.4 MB 4.0 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 8.8/16.4 MB 3.9 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 8.8/16.4 MB 3.9 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 8.9/16.4 MB 3.8 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 8.9/16.4 MB 3.8 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 8.9/16.4 MB 3.7 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 8.9/16.4 MB 3.7 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 8.9/16.4 MB 3.7 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 8.9/16.4 MB 3.7 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 8.9/16.4 MB 3.5 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 9.0/16.4 MB 3.5 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 9.0/16.4 MB 3.4 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 9.0/16.4 MB 3.4 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 9.0/16.4 MB 3.3 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 9.1/16.4 MB 3.3 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 9.1/16.4 MB 3.3 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 9.1/16.4 MB 3.2 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 9.1/16.4 MB 3.2 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 9.2/16.4 MB 3.2 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 9.2/16.4 MB 3.1 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 9.2/16.4 MB 3.1 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 9.3/16.4 MB 3.1 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 9.3/16.4 MB 3.1 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 9.4/16.4 MB 3.0 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 9.5/16.4 MB 3.0 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 9.6/16.4 MB 3.0 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 9.8/16.4 MB 3.0 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 9.9/16.4 MB 3.0 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 10.1/16.4 MB 3.1 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 10.2/16.4 MB 3.1 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 10.4/16.4 MB 3.0 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 10.5/16.4 MB 3.0 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 10.8/16.4 MB 3.0 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 11.0/16.4 MB 3.0 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 11.3/16.4 MB 3.0 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 11.7/16.4 MB 3.1 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 12.0/16.4 MB 3.1 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 12.3/16.4 MB 3.1 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 12.5/16.4 MB 3.1 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 12.9/16.4 MB 3.2 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 13.2/16.4 MB 3.2 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 13.6/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 13.8/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 14.0/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 14.3/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 14.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 14.6/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 14.8/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 14.9/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 15.3/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 15.6/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 15.7/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 15.9/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 16.4/16.4 MB 3.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.4/16.4 MB 2.5 MB/s eta 0:00:00\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "     l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/13.1 MB ? eta -:--:--━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.1/13.1 MB 3.6 MB/s eta 0:00:04╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.2/13.1 MB 3.5 MB/s eta 0:00:04━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.4/13.1 MB 3.7 MB/s eta 0:00:04━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.5/13.1 MB 3.3 MB/s eta 0:00:04━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.5/13.1 MB 2.9 MB/s eta 0:00:05━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.5/13.1 MB 2.5 MB/s eta 0:00:06━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/13.1 MB 2.2 MB/s eta 0:00:06━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/13.1 MB 2.0 MB/s eta 0:00:07━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/13.1 MB 1.8 MB/s eta 0:00:07━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/13.1 MB 1.8 MB/s eta 0:00:08━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/13.1 MB 1.6 MB/s eta 0:00:08━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/13.1 MB 1.6 MB/s eta 0:00:08━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/13.1 MB 1.5 MB/s eta 0:00:09━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/13.1 MB 1.4 MB/s eta 0:00:09━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/13.1 MB 1.4 MB/s eta 0:00:09━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/13.1 MB 1.4 MB/s eta 0:00:09━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/13.1 MB 1.4 MB/s eta 0:00:09━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/13.1 MB 1.4 MB/s eta 0:00:09━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/13.1 MB 1.4 MB/s eta 0:00:09━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/13.1 MB 1.4 MB/s eta 0:00:09━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/13.1 MB 1.4 MB/s eta 0:00:09━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/13.1 MB 1.4 MB/s eta 0:00:09━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/13.1 MB 1.4 MB/s eta 0:00:09━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/13.1 MB 839.1 kB/s eta 0:00:15━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/13.1 MB 839.1 kB/s eta 0:00:15━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/13.1 MB 839.1 kB/s eta 0:00:15━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/13.1 MB 839.1 kB/s eta 0:00:15━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.8/13.1 MB 743.7 kB/s eta 0:00:17━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.8/13.1 MB 746.2 kB/s eta 0:00:17━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.8/13.1 MB 746.3 kB/s eta 0:00:17━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.9/13.1 MB 757.7 kB/s eta 0:00:17━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.9/13.1 MB 766.7 kB/s eta 0:00:16━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.9/13.1 MB 765.0 kB/s eta 0:00:16━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/13.1 MB 753.7 kB/s eta 0:00:17━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/13.1 MB 753.7 kB/s eta 0:00:17━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/13.1 MB 753.7 kB/s eta 0:00:17━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/13.1 MB 710.9 kB/s eta 0:00:18━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/13.1 MB 710.9 kB/s eta 0:00:18━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/13.1 MB 676.9 kB/s eta 0:00:18━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/13.1 MB 680.6 kB/s eta 0:00:18━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/13.1 MB 692.4 kB/s eta 0:00:18━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/13.1 MB 709.2 kB/s eta 0:00:17━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/13.1 MB 744.8 kB/s eta 0:00:16━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/13.1 MB 749.6 kB/s eta 0:00:16━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/13.1 MB 742.8 kB/s eta 0:00:16━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/13.1 MB 742.8 kB/s eta 0:00:16━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/13.1 MB 735.5 kB/s eta 0:00:16━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/13.1 MB 735.5 kB/s eta 0:00:16━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/13.1 MB 738.6 kB/s eta 0:00:16━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/13.1 MB 745.8 kB/s eta 0:00:16━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/13.1 MB 819.0 kB/s eta 0:00:15━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/13.1 MB 833.2 kB/s eta 0:00:14━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/13.1 MB 869.0 kB/s eta 0:00:14━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/13.1 MB 878.1 kB/s eta 0:00:13━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/13.1 MB 949.0 kB/s eta 0:00:12━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/13.1 MB 1.0 MB/s eta 0:00:11━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/13.1 MB 1.1 MB/s eta 0:00:10━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/13.1 MB 1.3 MB/s eta 0:00:09━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/13.1 MB 1.3 MB/s eta 0:00:08━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/13.1 MB 1.3 MB/s eta 0:00:08━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/13.1 MB 1.4 MB/s eta 0:00:08━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/13.1 MB 1.4 MB/s eta 0:00:08━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.5/13.1 MB 1.4 MB/s eta 0:00:07━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.7/13.1 MB 1.5 MB/s eta 0:00:07━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.9/13.1 MB 1.6 MB/s eta 0:00:06━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.1/13.1 MB 1.6 MB/s eta 0:00:06━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/13.1 MB 1.7 MB/s eta 0:00:06━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/13.1 MB 1.8 MB/s eta 0:00:05━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 5.0/13.1 MB 1.9 MB/s eta 0:00:05━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/13.1 MB 1.9 MB/s eta 0:00:05━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 5.3/13.1 MB 1.9 MB/s eta 0:00:05━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 5.4/13.1 MB 2.0 MB/s eta 0:00:04━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 5.4/13.1 MB 2.0 MB/s eta 0:00:04━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 5.5/13.1 MB 2.0 MB/s eta 0:00:04━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 5.7/13.1 MB 2.0 MB/s eta 0:00:04━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 5.9/13.1 MB 2.0 MB/s eta 0:00:04━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 6.2/13.1 MB 2.1 MB/s eta 0:00:04━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 6.3/13.1 MB 2.1 MB/s eta 0:00:04━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 6.5/13.1 MB 2.2 MB/s eta 0:00:04━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 6.6/13.1 MB 2.2 MB/s eta 0:00:04━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 6.6/13.1 MB 2.1 MB/s eta 0:00:04━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 6.6/13.1 MB 2.1 MB/s eta 0:00:04━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 6.7/13.1 MB 2.1 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 6.8/13.1 MB 2.1 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 6.8/13.1 MB 2.1 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 6.9/13.1 MB 2.1 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 6.9/13.1 MB 2.1 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 6.9/13.1 MB 2.1 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 6.9/13.1 MB 2.0 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 7.0/13.1 MB 2.0 MB/s eta 0:00:04━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 7.0/13.1 MB 2.0 MB/s eta 0:00:04━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 7.0/13.1 MB 2.0 MB/s eta 0:00:04━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 7.1/13.1 MB 2.0 MB/s eta 0:00:04━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 7.1/13.1 MB 2.0 MB/s eta 0:00:04━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 7.2/13.1 MB 2.0 MB/s eta 0:00:04━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 7.2/13.1 MB 2.0 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 7.5/13.1 MB 2.0 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 7.6/13.1 MB 2.0 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 7.8/13.1 MB 2.0 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 8.1/13.1 MB 2.1 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 8.3/13.1 MB 2.1 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 8.5/13.1 MB 2.1 MB/s eta 0:00:03━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 8.8/13.1 MB 2.2 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 9.1/13.1 MB 2.3 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 9.3/13.1 MB 2.3 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 9.7/13.1 MB 2.4 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 10.1/13.1 MB 2.4 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 10.3/13.1 MB 2.5 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 10.6/13.1 MB 2.5 MB/s eta 0:00:02━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 10.8/13.1 MB 2.5 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 11.0/13.1 MB 3.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 11.2/13.1 MB 3.3 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 11.5/13.1 MB 3.7 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 11.6/13.1 MB 3.9 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 12.0/13.1 MB 4.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 12.1/13.1 MB 4.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 12.2/13.1 MB 4.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 12.4/13.1 MB 4.1 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 12.6/13.1 MB 4.1 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 12.7/13.1 MB 4.1 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.0/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 13.1/13.1 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 3.0 MB/s eta 0:00:00\n",
      "\u001b[?25hCollecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (2.32.3)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Collecting multitasking>=0.0.7\n",
      "  Downloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/site-packages (from yfinance) (4.3.6)\n",
      "Collecting frozendict>=2.3.4\n",
      "  Downloading frozendict-2.4.6-py311-none-any.whl (16 kB)\n",
      "Collecting peewee>=3.16.2\n",
      "  Downloading peewee-3.17.9.tar.gz (3.0 MB)\n",
      "     l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/3.0 MB ? eta -:--:--━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.3/3.0 MB 8.8 MB/s eta 0:00:01━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/3.0 MB 8.7 MB/s eta 0:00:01━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/3.0 MB 8.7 MB/s eta 0:00:01━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/3.0 MB 7.7 MB/s eta 0:00:01━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/3.0 MB 6.8 MB/s eta 0:00:01━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 1.2/3.0 MB 5.6 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 1.3/3.0 MB 5.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 1.3/3.0 MB 4.7 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 1.4/3.0 MB 4.3 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 1.4/3.0 MB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 1.5/3.0 MB 3.7 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 1.6/3.0 MB 3.5 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 1.7/3.0 MB 3.5 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 1.9/3.0 MB 3.6 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 2.2/3.0 MB 3.9 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 2.5/3.0 MB 4.1 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 2.7/3.0 MB 4.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 2.8/3.0 MB 4.1 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 3.0/3.0 MB 4.1 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 3.0/3.0 MB 4.1 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 3.0/3.0 MB 4.1 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 3.0/3.0 MB 4.1 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 3.0/3.0 MB 4.1 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 3.0/3.0 MB 4.1 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 3.0/3.0 MB 4.1 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 3.0/3.0 MB 4.1 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 2.9 MB/s eta 0:00:00\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/site-packages (from yfinance) (4.13.3)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "     l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/326.2 kB ? eta -:--:--━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 317.4/326.2 kB 10.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 317.4/326.2 kB 10.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 317.4/326.2 kB 10.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 326.2/326.2 kB 2.4 MB/s eta 0:00:00\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.56.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "     l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/4.9 MB ? eta -:--:--━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.3/4.9 MB 8.3 MB/s eta 0:00:01━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/4.9 MB 10.3 MB/s eta 0:00:01━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/4.9 MB 9.6 MB/s eta 0:00:01━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/4.9 MB 9.4 MB/s eta 0:00:01━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/4.9 MB 9.6 MB/s eta 0:00:01━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 2.0/4.9 MB 9.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 2.4/4.9 MB 9.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 2.7/4.9 MB 9.3 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 3.1/4.9 MB 9.4 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 3.4/4.9 MB 9.4 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 3.7/4.9 MB 9.2 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 4.1/4.9 MB 9.4 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 4.6/4.9 MB 9.4 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 4.9/4.9 MB 9.5 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 4.9/4.9 MB 9.5 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 4.9/4.9 MB 9.5 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 4.9/4.9 MB 9.5 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 4.9/4.9 MB 9.5 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 4.9/4.9 MB 9.5 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 6.8 MB/s eta 0:00:00\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "     l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/1.4 MB ? eta -:--:--━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.4/1.4 MB 11.6 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 0.6/1.4 MB 9.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 0.9/1.4 MB 8.8 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 1.2/1.4 MB 8.3 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 1.4/1.4 MB 8.5 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 1.4/1.4 MB 8.5 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 1.4/1.4 MB 8.5 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 1.4/1.4 MB 8.5 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 1.4/1.4 MB 8.5 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 4.2 MB/s eta 0:00:00\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/site-packages (from matplotlib) (11.1.0)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "     l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/107.7 kB ? eta -:--:--━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 102.4/107.7 kB 13.8 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 102.4/107.7 kB 13.8 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 102.4/107.7 kB 13.8 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107.7/107.7 kB 786.9 kB/s eta 0:00:00\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "     l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/346.8 kB ? eta -:--:--━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 337.9/346.8 kB 12.5 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 337.9/346.8 kB 12.5 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 337.9/346.8 kB 12.5 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.8/346.8 kB 2.5 MB/s eta 0:00:00\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Building wheels for collected packages: peewee\n",
      "  Building wheel for peewee (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peewee: filename=peewee-3.17.9-cp311-cp311-linux_x86_64.whl size=1030924 sha256=3e7fd3956517a1dbc06c7768a2aed6aeae23a7343619dfe245e3a5eb60d124e2\n",
      "  Stored in directory: /root/.cache/pip/wheels/ed/fd/80/48a915968e77972758d473495648213961ac6a251fff460d67\n",
      "Successfully built peewee\n",
      "Installing collected packages: pytz, peewee, multitasking, tzdata, python-dotenv, pyparsing, numpy, kiwisolver, frozendict, fonttools, cycler, pandas, contourpy, bs4, yfinance, matplotlib\n",
      "Successfully installed bs4-0.0.2 contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 frozendict-2.4.6 kiwisolver-1.4.8 matplotlib-3.10.1 multitasking-0.0.11 numpy-2.2.3 pandas-2.2.3 peewee-3.17.9 pyparsing-3.2.1 python-dotenv-1.0.1 pytz-2025.1 tzdata-2025.1 yfinance-0.2.54\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install yfinance matplotlib pytz numpy pandas python-dotenv requests bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core.tools import FunctionTool\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_stock(ticker: str) -> dict:  # type: ignore[type-arg]\n",
    "    import os\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import yfinance as yf\n",
    "    from pytz import timezone  # type: ignore\n",
    "\n",
    "    stock = yf.Ticker(ticker)\n",
    "\n",
    "    # Get historical data (1 year of data to ensure we have enough for 200-day MA)\n",
    "    end_date = datetime.now(timezone(\"UTC\"))\n",
    "    start_date = end_date - timedelta(days=365)\n",
    "    hist = stock.history(start=start_date, end=end_date)\n",
    "\n",
    "    # Ensure we have data\n",
    "    if hist.empty:\n",
    "        return {\"error\": \"No historical data available for the specified ticker.\"}\n",
    "\n",
    "    # Compute basic statistics and additional metrics\n",
    "    current_price = stock.info.get(\"currentPrice\", hist[\"Close\"].iloc[-1])\n",
    "    year_high = stock.info.get(\"fiftyTwoWeekHigh\", hist[\"High\"].max())\n",
    "    year_low = stock.info.get(\"fiftyTwoWeekLow\", hist[\"Low\"].min())\n",
    "\n",
    "    # Calculate 50-day and 200-day moving averages\n",
    "    ma_50 = hist[\"Close\"].rolling(window=50).mean().iloc[-1]\n",
    "    ma_200 = hist[\"Close\"].rolling(window=200).mean().iloc[-1]\n",
    "\n",
    "    # Calculate YTD price change and percent change\n",
    "    ytd_start = datetime(end_date.year, 1, 1, tzinfo=timezone(\"UTC\"))\n",
    "    ytd_data = hist.loc[ytd_start:]  # type: ignore[misc]\n",
    "    if not ytd_data.empty:\n",
    "        price_change = ytd_data[\"Close\"].iloc[-1] - ytd_data[\"Close\"].iloc[0]\n",
    "        percent_change = (price_change / ytd_data[\"Close\"].iloc[0]) * 100\n",
    "    else:\n",
    "        price_change = percent_change = np.nan\n",
    "\n",
    "    # Determine trend\n",
    "    if pd.notna(ma_50) and pd.notna(ma_200):\n",
    "        if ma_50 > ma_200:\n",
    "            trend = \"Upward\"\n",
    "        elif ma_50 < ma_200:\n",
    "            trend = \"Downward\"\n",
    "        else:\n",
    "            trend = \"Neutral\"\n",
    "    else:\n",
    "        trend = \"Insufficient data for trend analysis\"\n",
    "\n",
    "    # Calculate volatility (standard deviation of daily returns)\n",
    "    daily_returns = hist[\"Close\"].pct_change().dropna()\n",
    "    volatility = daily_returns.std() * np.sqrt(252)  # Annualized volatility\n",
    "\n",
    "    # Create result dictionary\n",
    "    result = {\n",
    "        \"ticker\": ticker,\n",
    "        \"current_price\": current_price,\n",
    "        \"52_week_high\": year_high,\n",
    "        \"52_week_low\": year_low,\n",
    "        \"50_day_ma\": ma_50,\n",
    "        \"200_day_ma\": ma_200,\n",
    "        \"ytd_price_change\": price_change,\n",
    "        \"ytd_percent_change\": percent_change,\n",
    "        \"trend\": trend,\n",
    "        \"volatility\": volatility,\n",
    "    }\n",
    "\n",
    "    # Convert numpy types to Python native types for better JSON serialization\n",
    "    for key, value in result.items():\n",
    "        if isinstance(value, np.generic):\n",
    "            result[key] = value.item()\n",
    "\n",
    "    # Generate plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(hist.index, hist[\"Close\"], label=\"Close Price\")\n",
    "    plt.plot(hist.index, hist[\"Close\"].rolling(window=50).mean(), label=\"50-day MA\")\n",
    "    plt.plot(hist.index, hist[\"Close\"].rolling(window=200).mean(), label=\"200-day MA\")\n",
    "    plt.title(f\"{ticker} Stock Price (Past Year)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Price ($)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save plot to file\n",
    "    os.makedirs(\"coding\", exist_ok=True)\n",
    "    plot_file_path = f\"coding/{ticker}_stockprice.png\"\n",
    "    plt.savefig(plot_file_path)\n",
    "    print(f\"Plot saved as {plot_file_path}\")\n",
    "    result[\"plot_file_path\"] = plot_file_path\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_analysis_tool = FunctionTool(analyze_stock, description=\"Analyze stock data and generate a plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_analysis_agent = AssistantAgent(\n",
    "    name=\"Stock_Analysis_Agent\",\n",
    "    model_client=get_model_client('llama'),\n",
    "    tools=[stock_analysis_tool],\n",
    "    description=\"Analyze stock data and generate a plot\",\n",
    "    system_message=\"Perform data analysis.\",\n",
    ")\n",
    "\n",
    "report_agent = AssistantAgent(\n",
    "    name=\"Report_Agent\",\n",
    "    model_client=get_model_client('deepseek'),\n",
    "    description=\"Generate a report based the search and results of stock analysis\",\n",
    "    system_message=\"You are a helpful assistant that can generate a comprehensive report on a given topic based on search and stock analysis. When you done with generating the report, reply with TERMINATE.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "team = RoundRobinGroupChat([stock_analysis_agent, report_agent], max_turns=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "Write a financial report on American airlines\n"
     ]
    },
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_async/connection.py:101\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_async/connection.py:78\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connect(request)\n\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_async/connection.py:124\u001b[39m, in \u001b[36mAsyncHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_backend.connect_tcp(**kwargs)\n\u001b[32m    125\u001b[39m     trace.return_value = stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_backends/auto.py:31\u001b[39m, in \u001b[36mAutoBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._init_backend()\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.connect_tcp(\n\u001b[32m     32\u001b[39m     host,\n\u001b[32m     33\u001b[39m     port,\n\u001b[32m     34\u001b[39m     timeout=timeout,\n\u001b[32m     35\u001b[39m     local_address=local_address,\n\u001b[32m     36\u001b[39m     socket_options=socket_options,\n\u001b[32m     37\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_backends/anyio.py:113\u001b[39m, in \u001b[36mAnyIOBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    108\u001b[39m exc_map = {\n\u001b[32m    109\u001b[39m     \u001b[38;5;167;01mTimeoutError\u001b[39;00m: ConnectTimeout,\n\u001b[32m    110\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    111\u001b[39m     anyio.BrokenResourceError: ConnectError,\n\u001b[32m    112\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43manyio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfail_after\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/contextlib.py:155\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: All connection attempts failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1595\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1594\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1595\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.send(\n\u001b[32m   1596\u001b[39m         request,\n\u001b[32m   1597\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_stream_response_body(request=request),\n\u001b[32m   1598\u001b[39m         **kwargs,\n\u001b[32m   1599\u001b[39m     )\n\u001b[32m   1600\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py:393\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_async_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/contextlib.py:155\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: All connection attempts failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m stream = report_agent.run_stream(task=\u001b[33m\"\u001b[39m\u001b[33mWrite a financial report on American airlines\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Console(stream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/autogen_agentchat/ui/_console.py:117\u001b[39m, in \u001b[36mConsole\u001b[39m\u001b[34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[39m\n\u001b[32m    113\u001b[39m last_processed: Optional[T] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    115\u001b[39m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[32m    119\u001b[39m         duration = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/autogen_agentchat/agents/_base_chat_agent.py:176\u001b[39m, in \u001b[36mBaseChatAgent.run_stream\u001b[39m\u001b[34m(self, task, cancellation_token)\u001b[39m\n\u001b[32m    174\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    175\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid message type in sequence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(msg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_messages_stream(input_messages, cancellation_token):\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, Response):\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m message.chat_message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py:416\u001b[39m, in \u001b[36mAssistantAgent.on_messages_stream\u001b[39m\u001b[34m(self, messages, cancellation_token)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_result, CreateResult)\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     model_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model_client.create(\n\u001b[32m    417\u001b[39m         llm_messages, tools=\u001b[38;5;28mself\u001b[39m._tools + \u001b[38;5;28mself\u001b[39m._handoff_tools, cancellation_token=cancellation_token\n\u001b[32m    418\u001b[39m     )\n\u001b[32m    420\u001b[39m \u001b[38;5;66;03m# Add the response to the model context.\u001b[39;00m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model_context.add_message(AssistantMessage(content=model_result.content, source=\u001b[38;5;28mself\u001b[39m.name))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/autogen_ext/models/openai/_openai_client.py:534\u001b[39m, in \u001b[36mBaseOpenAIChatCompletionClient.create\u001b[39m\u001b[34m(self, messages, tools, json_output, extra_create_args, cancellation_token)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    533\u001b[39m     cancellation_token.link_future(future)\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_beta_client:\n\u001b[32m    536\u001b[39m     result = cast(ParsedChatCompletion[Any], result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:1927\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1885\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1886\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1887\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1924\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1925\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   1926\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1927\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   1928\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1929\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   1930\u001b[39m             {\n\u001b[32m   1931\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   1932\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   1933\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   1934\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   1935\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   1936\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   1937\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   1938\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   1939\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   1940\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   1941\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   1942\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   1943\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   1944\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   1945\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   1946\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   1947\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   1948\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   1949\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   1950\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   1951\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   1952\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   1953\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   1954\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   1955\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   1956\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   1957\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   1958\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   1959\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   1960\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   1961\u001b[39m             },\n\u001b[32m   1962\u001b[39m             completion_create_params.CompletionCreateParams,\n\u001b[32m   1963\u001b[39m         ),\n\u001b[32m   1964\u001b[39m         options=make_request_options(\n\u001b[32m   1965\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   1966\u001b[39m         ),\n\u001b[32m   1967\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   1968\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1969\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   1970\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1862\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1849\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1850\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1857\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1858\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1859\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1860\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1861\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1862\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1556\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[39m\n\u001b[32m   1553\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1554\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1556\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m   1557\u001b[39m     cast_to=cast_to,\n\u001b[32m   1558\u001b[39m     options=options,\n\u001b[32m   1559\u001b[39m     stream=stream,\n\u001b[32m   1560\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1561\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1562\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1619\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1616\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered Exception\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1619\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry_request(\n\u001b[32m   1620\u001b[39m         input_options,\n\u001b[32m   1621\u001b[39m         cast_to,\n\u001b[32m   1622\u001b[39m         retries_taken=retries_taken,\n\u001b[32m   1623\u001b[39m         stream=stream,\n\u001b[32m   1624\u001b[39m         stream_cls=stream_cls,\n\u001b[32m   1625\u001b[39m         response_headers=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1626\u001b[39m     )\n\u001b[32m   1628\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1629\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1689\u001b[39m, in \u001b[36mAsyncAPIClient._retry_request\u001b[39m\u001b[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[39m\n\u001b[32m   1685\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m, options.url, timeout)\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m anyio.sleep(timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1689\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m   1690\u001b[39m     options=options,\n\u001b[32m   1691\u001b[39m     cast_to=cast_to,\n\u001b[32m   1692\u001b[39m     retries_taken=retries_taken + \u001b[32m1\u001b[39m,\n\u001b[32m   1693\u001b[39m     stream=stream,\n\u001b[32m   1694\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1695\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1619\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1616\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered Exception\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1619\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry_request(\n\u001b[32m   1620\u001b[39m         input_options,\n\u001b[32m   1621\u001b[39m         cast_to,\n\u001b[32m   1622\u001b[39m         retries_taken=retries_taken,\n\u001b[32m   1623\u001b[39m         stream=stream,\n\u001b[32m   1624\u001b[39m         stream_cls=stream_cls,\n\u001b[32m   1625\u001b[39m         response_headers=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1626\u001b[39m     )\n\u001b[32m   1628\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1629\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1689\u001b[39m, in \u001b[36mAsyncAPIClient._retry_request\u001b[39m\u001b[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[39m\n\u001b[32m   1685\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m, options.url, timeout)\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m anyio.sleep(timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1689\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m   1690\u001b[39m     options=options,\n\u001b[32m   1691\u001b[39m     cast_to=cast_to,\n\u001b[32m   1692\u001b[39m     retries_taken=retries_taken + \u001b[32m1\u001b[39m,\n\u001b[32m   1693\u001b[39m     stream=stream,\n\u001b[32m   1694\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1695\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1629\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1619\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry_request(\n\u001b[32m   1620\u001b[39m             input_options,\n\u001b[32m   1621\u001b[39m             cast_to,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1625\u001b[39m             response_headers=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1626\u001b[39m         )\n\u001b[32m   1628\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1631\u001b[39m log.debug(\n\u001b[32m   1632\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m, request.method, request.url, response.status_code, response.reason_phrase\n\u001b[32m   1633\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mAPIConnectionError\u001b[39m: Connection error."
     ]
    }
   ],
   "source": [
    "stream = report_agent.run_stream(task=\"Write a financial report on American airlines\")\n",
    "await Console(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
